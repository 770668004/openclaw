#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ“ä½œæ—¥å¿—ç®¡ç†æ¨¡å—
è®°å½•ä»£ç ä¿®æ”¹ã€JSONé…ç½®å˜æ›´ç­‰æ“ä½œï¼Œé‡è¦è®°å½•æ°¸ä¹…ä¿å­˜ï¼Œæ™®é€šè®°å½•1å¤©ååˆ é™¤
"""

import os
import json
import time
from datetime import datetime, timedelta
from pathlib import Path
import hashlib

class OperationLogManager:
    def __init__(self, workspace_path="/home/kousoyu/.openclaw/workspace"):
        self.workspace_path = Path(workspace_path)
        self.log_file = self.workspace_path / "OPERATION_LOG.md"
        self.cleanup_threshold = 24 * 3600  # 1å¤©ï¼ˆç§’ï¼‰
        
    def log_operation(self, operation_type, details, importance="normal", file_path=None):
        """
        è®°å½•æ“ä½œæ—¥å¿—
        Args:
            operation_type (str): æ“ä½œç±»å‹ (create/modify/delete/config/code/json)
            details (str): æ“ä½œè¯¦æƒ…
            importance (str): é‡è¦æ€§ (critical/high/normal/low)
            file_path (str): ç›¸å…³æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        """
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # ç”Ÿæˆæ“ä½œæ‘˜è¦
        operation_summary = self._generate_operation_summary(operation_type, details, file_path)
        
        log_entry = f"""## {timestamp} - {operation_type.upper()}
**é‡è¦æ€§**: {importance}
**æ–‡ä»¶**: {file_path or 'N/A'}
**æ‘˜è¦**: {operation_summary}

{details}

---
"""
        
        # è¿½åŠ åˆ°æ—¥å¿—æ–‡ä»¶
        with open(self.log_file, 'a', encoding='utf-8') as f:
            f.write(log_entry)
            
        print(f"âœ… æ“ä½œæ—¥å¿—å·²è®°å½•åˆ° {self.log_file}")
        
    def _generate_operation_summary(self, operation_type, details, file_path):
        """ç”Ÿæˆæ“ä½œæ‘˜è¦"""
        if operation_type == "code":
            return f"ä»£ç ä¿®æ”¹: {file_path}"
        elif operation_type == "json":
            return f"JSONé…ç½®å˜æ›´: {file_path}"
        elif operation_type == "create":
            return f"åˆ›å»ºæ–‡ä»¶: {file_path}"
        elif operation_type == "modify":
            return f"ä¿®æ”¹æ–‡ä»¶: {file_path}"
        elif operation_type == "delete":
            return f"åˆ é™¤æ–‡ä»¶: {file_path}"
        elif operation_type == "config":
            return f"é…ç½®å˜æ›´: {file_path}"
        else:
            # æˆªå–è¯¦æƒ…çš„å‰50ä¸ªå­—ç¬¦ä½œä¸ºæ‘˜è¦
            return details[:50] + "..." if len(details) > 50 else details
            
    def load_operation_log(self, days_back=7, importance_filter=None):
        """
        åŠ è½½æ“ä½œæ—¥å¿—
        Args:
            days_back (int): åŠ è½½å¤šå°‘å¤©å†…çš„æ—¥å¿—
            importance_filter (str): é‡è¦æ€§è¿‡æ»¤å™¨ (critical/high/normal/low)
        Returns:
            str: æ“ä½œæ—¥å¿—å†…å®¹
        """
        if not self.log_file.exists():
            return "æ— æ“ä½œæ—¥å¿—è®°å½•"
            
        cutoff_time = datetime.now() - timedelta(days=days_back)
        relevant_logs = []
        
        with open(self.log_file, 'r', encoding='utf-8') as f:
            content = f.read()
            
        # æŒ‰æ¡ç›®åˆ†å‰²
        entries = content.split('## ')
        for entry in entries[1:]:
            if not entry.strip():
                continue
                
            try:
                # æå–æ—¶é—´æˆ³
                timestamp_str = entry.split(' - ')[0]
                entry_time = datetime.strptime(timestamp_str, "%Y-%m-%d %H:%M:%S")
                
                # æ£€æŸ¥æ˜¯å¦åœ¨æ—¶é—´èŒƒå›´å†…
                if entry_time >= cutoff_time:
                    # æ£€æŸ¥é‡è¦æ€§è¿‡æ»¤
                    if importance_filter:
                        if f'**é‡è¦æ€§**: {importance_filter}' in entry:
                            relevant_logs.append(entry)
                    else:
                        relevant_logs.append(entry)
                        
            except ValueError:
                # æ—¶é—´æˆ³æ ¼å¼é”™è¯¯ï¼Œè·³è¿‡
                continue
                
        if not relevant_logs:
            return f"æœ€è¿‘{days_back}å¤©å†…æ— ç›¸å…³æ“ä½œæ—¥å¿—"
            
        return "## " + "## ".join(relevant_logs)
        
    def cleanup_old_logs(self):
        """æ¸…ç†è¿‡æœŸçš„æ“ä½œæ—¥å¿—ï¼ˆä¿ç•™é‡è¦æ—¥å¿—ï¼‰"""
        if not self.log_file.exists():
            return
            
        current_time = time.time()
        kept_logs = []
        
        with open(self.log_file, 'r', encoding='utf-8') as f:
            content = f.read()
            
        entries = content.split('## ')
        for entry in entries[1:]:
            if not entry.strip():
                continue
                
            try:
                timestamp_str = entry.split(' - ')[0]
                entry_time = datetime.strptime(timestamp_str, "%Y-%m-%d %H:%M:%S")
                entry_timestamp = entry_time.timestamp()
                
                # æ£€æŸ¥é‡è¦æ€§
                is_important = ('**é‡è¦æ€§**: critical' in entry or 
                              '**é‡è¦æ€§**: high' in entry)
                
                # ä¿ç•™é‡è¦æ—¥å¿—æˆ–1å¤©å†…çš„æ—¥å¿—
                if is_important or (current_time - entry_timestamp <= self.cleanup_threshold):
                    kept_logs.append(entry)
                else:
                    print(f"ğŸ§¹ æ¸…ç†è¿‡æœŸæ“ä½œæ—¥å¿—: {timestamp_str}")
                    
            except ValueError:
                # æ—¶é—´æˆ³æ ¼å¼é”™è¯¯ï¼Œä¿ç•™ä»¥é˜²ä¸‡ä¸€
                kept_logs.append(entry)
                
        # é‡å†™æ—¥å¿—æ–‡ä»¶
        with open(self.log_file, 'w', encoding='utf-8') as f:
            if kept_logs:
                f.write("## " + "## ".join(kept_logs))
                
        print(f"âœ… æ“ä½œæ—¥å¿—æ¸…ç†å®Œæˆï¼Œä¿ç•™ {len(kept_logs)} æ¡è®°å½•")
        
    def get_log_statistics(self):
        """è·å–æ—¥å¿—ç»Ÿè®¡ä¿¡æ¯"""
        if not self.log_file.exists():
            return "æ— æ“ä½œæ—¥å¿—"
            
        with open(self.log_file, 'r', encoding='utf-8') as f:
            content = f.read()
            
        entries = content.split('## ')[1:]
        total_logs = len(entries)
        
        # ç»Ÿè®¡å„ç±»å‹æ“ä½œ
        operation_types = {}
        importance_levels = {"critical": 0, "high": 0, "normal": 0, "low": 0}
        
        for entry in entries:
            # ç»Ÿè®¡æ“ä½œç±»å‹
            lines = entry.strip().split('\n')
            if lines:
                first_line = lines[0]
                op_type = first_line.split(' - ')[1].lower() if ' - ' in first_line else 'unknown'
                operation_types[op_type] = operation_types.get(op_type, 0) + 1
                
            # ç»Ÿè®¡é‡è¦æ€§
            for level in importance_levels.keys():
                if f'**é‡è¦æ€§**: {level}' in entry:
                    importance_levels[level] += 1
                    break
                    
        stats = f"""### æ“ä½œæ—¥å¿—ç»Ÿè®¡:
- æ€»è®°å½•æ•°: {total_logs}
- æ“ä½œç±»å‹åˆ†å¸ƒ: {operation_types}
- é‡è¦æ€§åˆ†å¸ƒ: {importance_levels}
"""
        return stats

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    log_manager = OperationLogManager()
    
    # è®°å½•æ“ä½œæ—¥å¿—
    log_manager.log_operation(
        "create",
        "åˆ›å»ºäº†multi-memory-manageræŠ€èƒ½ç›®å½•å’Œç›¸å…³æ–‡ä»¶",
        importance="high",
        file_path="/home/kousoyu/.openclaw/workspace/skills/multi-memory-manager/"
    )
    
    log_manager.log_operation(
        "code",
        "å®ç°äº†core_memory.pyæ¨¡å—ï¼Œå¤„ç†æ ¸å¿ƒè®°å¿†ç®¡ç†",
        importance="normal",
        file_path="/home/kousoyu/.openclaw/workspace/skills/multi-memory-manager/core_memory.py"
    )
    
    # åŠ è½½æœ€è¿‘7å¤©çš„æ—¥å¿—
    recent_logs = log_manager.load_operation_log(days_back=7)
    print("æœ€è¿‘æ“ä½œæ—¥å¿—:")
    print(recent_logs)
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = log_manager.get_log_statistics()
    print("\næ—¥å¿—ç»Ÿè®¡:")
    print(stats)
    
    # æ¸…ç†è¿‡æœŸæ—¥å¿—
    log_manager.cleanup_old_logs()